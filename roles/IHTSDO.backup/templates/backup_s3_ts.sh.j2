#!/bin/bash

cp_cmd="/usr/bin/aws s3 cp "

# The S3 backup dir
#s3_backup_dir={{s3_backup_dir | default('')}}

# The following variables should be set by editing this script before running it:
# The S3 backup dir
#s3_backup_dir="{{termserv_s3_backup_dir | default('')}}"
#s3_backup_dir_full="s3://$s3_backup_dir"
#s3_backup_region_base="{{termserv_s3_backup_region | default('')}}"
#s3_numbackups=5
#s3_bk_list=s3list.txt

db_host={{backup_db_host}}
db_port={{backup_db_port}}
db_user={{backup_db_user}}
db_password={{backup_db_password}} 
db_include=({% for db in backup_db_names %}"{{db}}" {% endfor %})


DAILY="daily"
WEEKLY="weekly"
MONTHLY="monthly"
YEARLY="yearly"

#DAY_OF_WEEK="Saturday"
#DAY_OF_MONTH="01"
#DAY_OF_YEAR="001"

DAY_OF_WEEK="{{backup_DAY_OF_WEEK}}"
DAY_OF_MONTH="{{backup_DAY_OF_MONTH}}"
DAY_OF_YEAR="{{backup_DAY_OF_YEAR}}"

HOST_NAME="{{backup_hostname}}"
APP_NAME="{{backup_app_name}}"

# The timestamp suffix for the top-level directory, eg. 20120904_1021
#CURRENT_DATE=`date +%Y%m%d_%H%M`
# The S3 backup dir
s3_backup_dir="{{ backup_s3_backup_dir | default('') }}"
s3_backup_dir_prefix="s3://$s3_backup_dir"
s3_backup_dir_suffix="$HOST_NAME/$APP_NAME/"

s3_backup_dir_daily="$s3_backup_dir_prefix/$DAILY/$s3_backup_dir_suffix"
s3_backup_dir_weekly="$s3_backup_dir_prefix/$WEEKLY/$s3_backup_dir_suffix"
s3_backup_dir_monthly="$s3_backup_dir_prefix/$MONTHLY/$s3_backup_dir_suffix"
s3_backup_dir_yearly="$s3_backup_dir_prefix/$YEARLY/$s3_backup_dir_suffix"

s3_backup_region_base="{{ backup_s3_backup_region | default('') }}"


s3_backup_region=""
if [ "x$s3_backup_region_base" != "x" ]; then
      s3_backup_region="--region $s3_backup_region_base"
      echo "setting backup region to $s3_backup_region"
fi

# The timestamp suffix for the top-level directory, eg. 20120904_1021
CURRENT_DATE=`date +%Y%m%d_%H%M`

indexes=indexes
ts_indexes_dir={{ts_indexes_dir}}
ts_resources_dir={{ts_res_dir}}
ts_template_store={{ts_template_store}}

# The starting directory
#backup_dir=ts_backup_dir

# The dir where zips go
#tmp_backup_dir=$backup_dir/tmp

# The working directory and the resulting archive file prefix
#ARCHIVE_PREFIX="snowowl_$(hostname -s)_$CURRENT_DATE"

# The absolute path to the above
#ABSOLUTE_ARCHIVE_PREFIX="$tmp_backup_dir/$ARCHIVE_PREFIX/dataset"
# The path to the db dump status file (records the db dump status)
#ABSOLUTE_DBDUMP_STATUS=$ABSOLUTE_ARCHIVE_PREFIX/$tmp_dbdump_file

# The starting directory
#backup_dir={{ backup_dir }}

# The dir where zips go
tmp_backup_dir="{{backup_dir_tmp}}"

# The working directory and the resulting archive file prefix
ARCHIVE_PREFIX="{{backup_archive_prefix}}"

# The absolute path to the above
ABSOLUTE_ARCHIVE_PREFIX="{{backup_archive_dir}}"
# The path to the db dump status directory (records the db dump status)
ABSOLUTE_DBDUMP_STATUS_DIR=$ABSOLUTE_ARCHIVE_PREFIX/dumpstatus
# Archive
ARCHIVE_FILE="{{backup_archive_file}}"





# The connection timeout for HTTP requests in seconds.
CONNECTION_TIMEOUT_SECONDS=5

# The time to wait in seconds after start mysql dump before unlocking repo.
REPO_WAIT_SECONDS=5

# The username used for creating hot backups (should be a valid Snow Owl user)
SNOWOWL_USERNAME="{{ backup_rest_user }}"

# The password for the user given above
SNOWOWL_PASSWORD="{{ backup_rest_password }}"

# The base URL of the REST services to use
BASE_URL="{{ base_url }}"

# The base URL for administrative services
ADMIN_BASE_URL="$BASE_URL/admin"

# The number of milliseconds to wait before giving up trying to lock the complete repository.
LOCK_TIMEOUT_MILLIS=30000

# Prints a message to stdout with the current date and time.
echo_date() {
  echo -e "[`date +\"%Y-%m-%d %H:%M:%S\"`] $@"
}

# Prints an error message to stderr and exits the script with a non-zero status.
error_exit() {
  echo -e "[`date +\"%Y-%m-%d %H:%M:%S\"`] $@" >&2
  exit 1
}

# Checks input arguments and test whether the script is ready to be executed.
check_arguments() {
  if [ "x$tmp_backup_dir" = "x" ]; then
    error_exit "Please set the variable backup_dir before running this script. Exiting with error."
  fi
  if [ "x$s3_backup_dir" = "x" ]; then
    error_exit "Please set the variable s3_backup_dir before running this script. Exiting with error."
  fi  
  if [ "x$DAY_OF_WEEK" = "x" ]; then
    error_exit "Please set the variable DAY_OF_WEEK before running this script. Exiting with error."
  fi
  if [ "x$DAY_OF_MONTH" = "x" ]; then
    error_exit "Please set the variable DAY_OF_MONTH before running this script. Exiting with error."
  fi  
  if [ "x$DAY_OF_YEAR" = "x" ]; then
    error_exit "Please set the variable DAY_OF_YEAR before running this script. Exiting with error."
  fi  
  if [ "x$HOST_NAME" = "x" ]; then
    error_exit "Please set the variable HOST_NAME before running this script. Exiting with error."
  fi  
  if [ "x$APP_NAME" = "x" ]; then
    error_exit "Please set the variable APP_NAME before running this script. Exiting with error."
  fi  
#  if [ "x$db_type" = "x" ]; then
#    error_exit "Please set the variable db_type before running this script. Exiting with error."
#  fi
#  if [ "$db_type" != "none" ]; then
  # If not none check db vars are set
  if [ "x$db_host" = "x" ]; then
    error_exit "Please set the variable db_host before running this script. Exiting with error."
  fi

  if [ "x$db_port" = "x" ]; then
    error_exit "Please set the variable db_port before running this script. Exiting with error."
  fi
  
  if [ "x$db_include" = "x" ]; then
    error_exit "Please set the variable db_include before running this script. Exiting with error."
  fi

  if [ "x$db_user" = "x" ]; then
    error_exit "Please set the variable db_user before running this script. Exiting with error."
  fi
  
  if [ "x$db_password" = "x" ]; then
    error_exit "Please set the variable db_password before running this script. Exiting with error."
  fi
# fi
}

# Invokes curl to make an HTTP request to the server; stores returned message and HTTP status code in output variables.
rest_call() {
  CURL_OUTPUT=`curl -q --fail --silent --show-error --connect-timeout "$CONNECTION_TIMEOUT_SECONDS" --user "$SNOWOWL_USERNAME:$SNOWOWL_PASSWORD" --write-out "\n%{http_code}" "$@"`
  CURL_MESSAGE=`echo "$CURL_OUTPUT" | head -n-1`
  CURL_HTTP_STATUS=`echo "$CURL_OUTPUT" | tail -n1`
}

# Tries to acquire a global lock that prevents writing to any of the terminology stores on any available branch.
lock_all_repositories() {
  echo_date "Locking repositories..."
  rest_call --data-urlencode "timeoutMillis=$LOCK_TIMEOUT_MILLIS" "$ADMIN_BASE_URL/repositories/lock"
}

# Removes the lock from all repositories.
unlock_all_repositories() {
  echo_date "Unlocking repositories..."
  rest_call --data-urlencode "" "$ADMIN_BASE_URL/repositories/unlock"
}

# Cleans up the global repository lock and exits with an error.
unlock_all_repositories_and_exit() {
  unlock_all_repositories
  error_exit "$1"
}

# Locks the specified repository, preventing write access to all of its branches.
lock_repository() {
  echo_date "Locking repository $REPOSITORY..."
  rest_call --data-urlencode "" "$ADMIN_BASE_URL/repositories/$REPOSITORY/lock"

  if [ "$CURL_HTTP_STATUS" -ne "204" ]; then
    unlock_all_repositories_and_exit "Couldn't lock repository $REPOSITORY. Exiting with error."
  fi
}

# Unlocks the specified repository if it was already locked.
unlock_repository() {
  echo_date "Unlocking repository $REPOSITORY..."
  rest_call --data-urlencode "" "$ADMIN_BASE_URL/repositories/$REPOSITORY/unlock"
}

# Unlocks the specified repository, cleans up the global repository lock and exits with an error.
unlock_repository_and_exit() {
  unlock_repository
  unlock_all_repositories_and_exit "$1"
}

# Main script starts here.
main() {
  echo_date "----------------------------"
  check_arguments
  mkbkdir
  #clear out dbdump status dir
  rm -rf $ABSOLUTE_DBDUMP_STATUS_DIR
  mkdir -pv $ABSOLUTE_DBDUMP_STATUS_DIR
  lock_all_repositories
  #echo_date "Create backup destination directory '$ABSOLUTE_ARCHIVE_PREFIX'."
  #mkdir -pv "$ABSOLUTE_ARCHIVE_PREFIX" || unlock_all_repositories_and_exit "Couldn't create directory '$ABSOLUTE_ARCHIVE_PREFIX'. Exiting with error."
  echo_date "Starting backup; sending message to connected users."
  #echo_date "Rsync'ing indexes..."
  rsyncindexes
  echo_date "dumping mysql..."
     for db in ${db_include[*]};
        do  
        echo "db = "${db}
        mysql_dump ${db} &
     done
    
    sleep $REPO_WAIT_SECONDS
    unlock_all_repositories
    wait
    
   echo_date "checking mysqldump ok"
     for db in ${db_include[*]};
        do  
        echo "db = "${db}
        testdbdumpchild ${db}
     done
    
   
    echo_date "copying from rsync folder..."
    copyfromrsyncdir
    zip_backup
    export2net

  echo_date "Finished successfully."
  

  exit 0
}

testvar(){

echo "testvar var = $1"

}

testdbdumpchild(){

dbname=$1;  
db_status=$ABSOLUTE_DBDUMP_STATUS_DIR/$dbname.txt
echo_date "checking mysql status at $db_status " 

#First see file exists   

 
if [ -f $db_status ]; then
   echo "File $db_status exists."
   read dbdump_status < $db_status
   echo "The exit status of the dbdump process was $dbdump_status"
    if [ $dbdump_status -ne 0 ] ; then
    echo "Exiting as dbdump_status is not 0 but was instead $dbdump_status"
    unlock_all_repositories_and_exit "Exiting as dbdump_status is not 0 but was instead $dbdump_status"
    fi
else
   unlock_all_repositories_and_exit "File $db_status does not exist. Error so exiting"
fi
}

export2net(){
 if [ -f ~/.aws/config ];
    then
      movetoaws

 fi
}

zip_backup(){
  echo_date "Creating archive..."
  echo_date "ARCHIVE_PREFIX.zip = $ARCHIVE_PREFIX.zip"
  echo_date "ARCHIVE_PREFIX = $ARCHIVE_PREFIX"
  cd "$tmp_backup_dir"
  zip --recurse-paths --move --test "$ARCHIVE_PREFIX.zip" "$ARCHIVE_PREFIX" || unlock_all_repositories_and_exit "Archive creation failed; the backup is incomplete. Exiting with error."

}

mysql_dump(){
dbname=$1;  
db_out=$dbname.sql
db_status=$ABSOLUTE_DBDUMP_STATUS_DIR/$dbname.txt
echo_date "dumping mysql db $dbname to $db_out with status written to $db_status "  

/usr/bin/mysqldump -u{{backup_db_user}} -p{{ backup_db_password }} -h {{ backup_db_host }} -P {{ backup_db_port }} --max_allowed_packet=512M  --quick --single-transaction $dbname > $ABSOLUTE_ARCHIVE_PREFIX/$db_out
EXITCODE=$?
echo "$EXITCODE" >> "$db_status"
if [ $EXITCODE -ne 0 ] ; then 
      echo "DB backup for $dbname failed with exit code $EXITCODE "
 else
      echo "DB Backup of $dbname finished ok"
 fi              
  echo_date "Done backing up DB $dbname."

}

rsyncindexes(){
mkdir -pv {{ rsync_dir_new }}
/usr/bin/rsync  -avR --delete --no-compress $ts_indexes_dir {{ rsync_dir_new }}
/usr/bin/rsync  -avR --delete --no-compress --no-links $ts_resources_dir {{ rsync_dir_new }}
/usr/bin/rsync  -avR --delete --no-compress --no-links $ts_template_store {{ rsync_dir_new }}
}

copyfromrsyncdir(){
cp -R {{ rsync_dir_new }}/* $ABSOLUTE_ARCHIVE_PREFIX
}


mkbkdir(){
# first delete if there
echo_date "Deleting $tmp_backup_dir and $ABSOLUTE_ARCHIVE_PREFIX"
rm -rf $tmp_backup_dir
rm -rf $ABSOLUTE_ARCHIVE_PREFIX
echo_date "Create tmp backup destination directory '$tmp_backup_dir'."
mkdir -pv $tmp_backup_dir
echo_date "Create backup destination directory '$ABSOLUTE_ARCHIVE_PREFIX'."
mkdir -pv "$ABSOLUTE_ARCHIVE_PREFIX" || error_exit "Couldn't create directory '$ABSOLUTE_ARCHIVE_PREFIX'. Exiting with error.";
if [ "x$backup_dir_owner" != "x" ]; then
chown -vR $backup_dir_owner:$backup_dir_owner $tmp_backup_dir
fi
}

movetoaws(){
echo "s3_backup_dir_prefix = "$s3_backup_dir_prefix;
echo "s3_backup_dir_daily = "$s3_backup_dir_daily;
echo "s3_backup_dir_weekly = "$s3_backup_dir_weekly;
echo "s3_backup_dir_monthly = "$s3_backup_dir_monthly;
echo "s3_backup_dir_yearly = "$s3_backup_dir_yearly;

LANG=C DOW=$(date +"%A")
echo "Day of Week ="$DOW
DOM=$(date +"%d")
echo "Day of Month ="$DOM
DOY=$(date +"%j")
echo "Day of Year ="$DOY

echo "temp dir = $tmp_backup_dir"
echo "ARCHIVE_FILE = $ARCHIVE_FILE"

#Always copy to daily

# /usr/bin/aws s3 cp $i $s3_backup_dir_full $s3_backup_region;
echo "Making a daily back up using $cp_cmd $tmp_backup_dir/$ARCHIVE_FILE $s3_backup_dir_daily $s3_backup_region" 
eval $cp_cmd $tmp_backup_dir/$ARCHIVE_FILE $s3_backup_dir_daily $s3_backup_region;
echo "Made a daily back up"
#Copy from daily to weekly
if [ "$DOW" = "$DAY_OF_WEEK" ];
then
echo "Making a weekly back up"
eval $cp_cmd $s3_backup_dir_daily$ARCHIVE_FILE $s3_backup_dir_weekly $s3_backup_region;
fi
#Copy from daily to monthly
if [ "$DOM" = "$DAY_OF_MONTH" ];
then
echo "Making a Monthly back up"
eval $cp_cmd $s3_backup_dir_daily$ARCHIVE_FILE $s3_backup_dir_monthly $s3_backup_region;
fi
#Copy from daily to yearly
#if 1st of year copy to yearly.
if [ "$DOY" = "$DAY_OF_YEAR" ];
then
echo "Making a yearly back up"
eval $cp_cmd $s3_backup_dir_daily$ARCHIVE_FILE $s3_backup_dir_yearly $s3_backup_region;
fi

echo "Finished copying to AWS"

}

# Ensures that only a single instance is running at any time
LOCKFILE="{{backup_script_dir}}/instance.lock"

(
        flock -n 200 || unlock_all_repositories_and_exit "Another backup script is already running. Exiting with error."
        trap "rm $LOCKFILE" EXIT
        main
) 200> $LOCKFILE
